{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwojmN7WsQ23/R5r+JhrEY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "TBDvNoy9hSkB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3"
      ],
      "metadata": {
        "id": "Dzm4MRH1o7jC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0ZxmLiFycE_x"
      },
      "outputs": [],
      "source": [
        "def logistic_mini_sgd(X, y, batch_size, learning_rate, max_iter=1000):\n",
        "    n_samples, n_features = X.shape\n",
        "    w = np.zeros(n_features)  # initialize weights with value zero\n",
        "\n",
        "    def sigmoid(z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def cross_entropy_loss(y_true, y_pred):\n",
        "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "    # training loop\n",
        "    for i in range(max_iter):\n",
        "        # shuffle the data for each iteration\n",
        "        indices = np.arange(n_samples)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        for batch_start in range(0, n_samples, batch_size):\n",
        "            batch_indices = indices[batch_start:batch_start + batch_size]\n",
        "            X_batch = X[batch_indices]\n",
        "            y_batch = y[batch_indices]\n",
        "\n",
        "            # predict using sigmoid\n",
        "            y_pred = sigmoid(np.dot(X_batch, w))\n",
        "\n",
        "            # find gradients\n",
        "            error = y_pred - y_batch\n",
        "            grad = np.dot(X_batch.T, error) / batch_size  # Average gradient over batch\n",
        "\n",
        "            # update weights\n",
        "            w = w - learning_rate * grad\n",
        "\n",
        "    return w"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 4"
      ],
      "metadata": {
        "id": "1t4k06M6o_6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a)"
      ],
      "metadata": {
        "id": "EBxjZoW9pDtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bc = load_breast_cancer()"
      ],
      "metadata": {
        "id": "5xPs6FuXgTda"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b)"
      ],
      "metadata": {
        "id": "9-KIyD6vpFVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = bc.data\n",
        "y = bc.target\n",
        "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.75) # split the data to get training dataset\n",
        "x_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=0.4) # split the remaining to get validation and test datasets\n",
        "# scale = StandardScaler()\n",
        "# X_train = scale.fit_transform(X_train)\n",
        "# x_val = scale.transform(x_val)\n",
        "# X_test = scale.transform(X_test)"
      ],
      "metadata": {
        "id": "462aykregewl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c)"
      ],
      "metadata": {
        "id": "AIMOh72ppIzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# combine training and validation sets\n",
        "y_train_val = np.concatenate([y_train, y_val])\n",
        "\n",
        "# count\n",
        "count_train_val = np.bincount(y_train_val)\n",
        "\n",
        "print(f\"Class distribution in training (+ validation) set: {count_train_val}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2GgjawCh4II",
        "outputId": "d261deaf-e463-483a-8748-24b40ff68aa4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution in training (+ validation) set: [194 317]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "d)"
      ],
      "metadata": {
        "id": "iaIyXBOZpKFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = logistic_mini_sgd(X_train, y_train, batch_size=10, learning_rate=0.01)\n",
        "\n",
        "print(\"Trained Weights:\", weights)\n",
        "\n",
        "weights = logistic_mini_sgd(X_train, y_train, batch_size=32, learning_rate=0.01)\n",
        "\n",
        "print(\"Trained Weights:\", weights)\n",
        "\n",
        "weights = logistic_mini_sgd(X_train, y_train, batch_size=32, learning_rate=0.001)\n",
        "\n",
        "print(\"Trained Weights:\", weights)\n",
        "\n",
        "weights = logistic_mini_sgd(X_train, y_train, batch_size=5, learning_rate=0.0001)\n",
        "\n",
        "print(\"Trained Weights:\", weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shMzEBz6ijKX",
        "outputId": "8b09e5cf-3cb3-4a53-8cb6-03a4c1f0a682"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-0a169bbda1e1>:8: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-z))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained Weights: [ 2.13286971e+01 -2.00860450e+01  7.95153773e+01  7.33196947e+00\n",
            " -1.20079876e-01 -1.45582088e+00 -2.24860375e+00 -8.38478440e-01\n",
            " -2.58668001e-01  1.99581772e-04  5.16317250e-01 -4.42954485e-01\n",
            " -3.06439290e+00 -1.66622522e+01 -3.46137649e-02 -4.26790888e-01\n",
            " -5.55277669e-01 -1.11695371e-01 -1.04783249e-01 -3.87077179e-02\n",
            "  2.19830717e+01 -4.21094100e+01  3.25960194e+01 -1.69557094e+01\n",
            " -3.61789739e-01 -5.28348372e+00 -6.54583365e+00 -1.65310737e+00\n",
            " -1.13310464e+00 -4.04023498e-01]\n",
            "Trained Weights: [ 8.88974197e+00 -7.33865423e+00  3.85244111e+01  6.30000445e+00\n",
            " -8.89129999e-03 -4.38387600e-01 -7.13951634e-01 -2.65855890e-01\n",
            " -3.63688950e-02  1.93296815e-02  1.68016194e-01 -2.63721259e-01\n",
            " -9.94750187e-01 -1.14638262e+01 -9.80267436e-03 -1.33738457e-01\n",
            " -1.73925720e-01 -3.40526929e-02 -2.97209460e-02 -1.17915448e-02\n",
            "  9.28361919e+00 -1.48735041e+01  2.48372853e+01 -1.24930295e+01\n",
            " -7.75420517e-02 -1.62538191e+00 -2.05412783e+00 -5.09473967e-01\n",
            " -2.91327365e-01 -1.03809468e-01]\n",
            "Trained Weights: [ 8.91767765e-01 -7.24429951e-01  3.87388056e+00  5.45118945e-01\n",
            " -9.12366290e-04 -4.37465108e-02 -7.09685490e-02 -2.65063263e-02\n",
            " -3.50812185e-03  1.96794167e-03  1.69618032e-02 -2.73433857e-02\n",
            " -1.07249769e-01 -1.17298650e+00 -9.67405998e-04 -1.31087574e-02\n",
            " -1.70929146e-02 -3.37744112e-03 -2.90564832e-03 -1.13456329e-03\n",
            "  9.33332654e-01 -1.47445078e+00  2.48705234e+00 -1.23986386e+00\n",
            " -7.77512508e-03 -1.61921336e-01 -2.04023066e-01 -5.08153081e-02\n",
            " -2.87905663e-02 -1.02984141e-02]\n",
            "Trained Weights: [ 3.82823716e-01 -3.12457744e-01  1.32717790e+00  1.02950219e-01\n",
            " -2.92587986e-03 -2.93797423e-02 -4.44576036e-02 -1.66239312e-02\n",
            " -6.03283465e-03 -3.51111072e-04  9.85904947e-03 -1.49823353e-03\n",
            " -6.06022252e-02 -2.29849374e-01 -7.04482000e-04 -8.56012240e-03\n",
            " -1.10637598e-02 -2.25110449e-03 -2.17340299e-03 -7.89613933e-04\n",
            "  3.92750749e-01 -6.94706640e-01  3.84035248e-01 -2.55436277e-01\n",
            " -7.78343145e-03 -1.05402877e-01 -1.29387547e-01 -3.29477997e-02\n",
            " -2.38153297e-02 -8.38344953e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "e)"
      ],
      "metadata": {
        "id": "mWpPm_IKpNRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def predict(X, w):\n",
        "    probs = sigmoid(np.dot(X, w))\n",
        "    return np.where(probs >= 0.5, 1, 0)  # Threshold at 0.5\n",
        "\n",
        "# test prediction\n",
        "y_pred_test = predict(X_test, weights)\n",
        "\n",
        "# performance metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_test)\n",
        "precision = precision_score(y_test, y_pred_test)\n",
        "recall = recall_score(y_test, y_pred_test)\n",
        "f1 = f1_score(y_test, y_pred_test)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUXJU_AEkXf3",
        "outputId": "3c956911-0e5e-49b6-fffc-65d573256b6f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9482758620689655\n",
            "Precision: 0.9302325581395349\n",
            "Recall: 1.0\n",
            "F1-score: 0.963855421686747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "f)\n",
        "\n",
        "The trained weights in (d) show that smaller batch sizes and learning rates generally lead to better convergence.\n",
        "\n",
        "With an accuracy of 94.82%, the model is classifying most points correctly.\n",
        "The precision is 0.93, indicating that that when the model predicts a positive class, it is correct about 93% of the time.\n",
        "The recall is 1.0, meaning that the model has successfully identifies all actual positive instances.\n",
        "With a 0.964 F1-score, the model has a good overall performance.\n",
        "\n",
        "The recall is 1.0, but with a slightly lower precision, the model might have slightly overpredicted the positive class. **bold text**"
      ],
      "metadata": {
        "id": "GyNxWiqSmHIb"
      }
    }
  ]
}